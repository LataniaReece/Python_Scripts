{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ChurnData.csv', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Listing all categorical variables: \n",
    "# for variable in df: \n",
    "#     if df[variable].dtype == 'O':\n",
    "#         print(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating function that names the dummy variables as I want\n",
    "def dummyWithNames(variable, prefix_name):\n",
    "    variable = pd.get_dummies(df[variable], drop_first = True, prefix= prefix_name)\n",
    "    return variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Executing function on churn data saving into check \n",
    "dummies = []\n",
    "for variable in df: \n",
    "    if df[variable].dtype == 'O':\n",
    "        variable = dummyWithNames(variable, str(variable))\n",
    "        dummies.append(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.concat(dummies, axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([dummies, df], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 70)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = merged.drop(['churn', 'voice_mail_plan', 'state', 'area_code', 'international_plan'], axis = 1)\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(merged.drop(['churn_yes'], axis = 1),\n",
    "                                                    merged.churn_yes, \n",
    "                                                    test_size = 0.3, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train.values\n",
    "y = y_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA with Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalize the data \n",
    "from sklearn.preprocessing import StandardScaler #normalization \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.15294382, -0.13970132, -0.12635334, ..., -0.18480092,\n",
       "         0.89971603, -0.41649683],\n",
       "       [-0.15294382, -0.13970132, -0.12635334, ...,  0.2197039 ,\n",
       "         1.73008302, -1.18292108],\n",
       "       [-0.15294382, -0.13970132, -0.12635334, ..., -0.18480092,\n",
       "         2.26580365,  0.34992741],\n",
       "       ...,\n",
       "       [-0.15294382, -0.13970132, -0.12635334, ...,  1.43321837,\n",
       "        -0.69405287, -1.18292108],\n",
       "       [ 6.53834842, -0.13970132, -0.12635334, ...,  1.43321837,\n",
       "        -1.0556643 , -0.41649683],\n",
       "       [-0.15294382, -0.13970132, -0.12635334, ..., -0.18480092,\n",
       "         0.06934904, -1.18292108]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create new object with scaled data \n",
    "scaled_data = scaler.transform(X)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 69)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reminder of number of variables in dataset\n",
    "scaled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Explained Variance:  0.9029107050220371\n",
      "Variance per PC:  [0.03048895 0.02979029 0.02963843 0.02900095 0.02782798 0.02322009\n",
      " 0.0171704  0.01666466 0.01632486 0.01624009 0.01622069 0.01583487\n",
      " 0.01553846 0.0149656  0.0149235  0.01484958 0.01484257 0.01483882\n",
      " 0.01483408 0.01482455 0.01482224 0.01481843 0.01481317 0.01480942\n",
      " 0.0148052  0.01480195 0.01479708 0.01479479 0.01479019 0.01478638\n",
      " 0.01478082 0.01477911 0.01477609 0.0147714  0.01476967 0.01476798\n",
      " 0.01476355 0.01476317 0.01476003 0.01475624 0.01475174 0.01474849\n",
      " 0.01474596 0.01474095 0.01473584 0.01473216 0.01472792 0.01471311\n",
      " 0.01467813 0.01463212 0.01456407 0.01448807 0.0144251  0.0143181\n",
      " 0.01414261]\n"
     ]
    }
   ],
   "source": [
    "#Finding Components and variance explained\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 55)\n",
    "X = pca.fit_transform(scaled_data)\n",
    "print('Total Explained Variance: ', pca.explained_variance_ratio_.sum())\n",
    "print('Variance per PC: ', pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating df with PC and known labels\n",
    "pc_df = pd.DataFrame(data = X, \n",
    "        columns = range(1,56,1))\n",
    "dummy_pca_data_X =pc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Without Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = []\n",
    "for column in merged: \n",
    "    if (merged[column].dtype == 'float64') | (merged[column].dtype == 'int64')  :\n",
    "        keep_columns.append(column)\n",
    "# keep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 70)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dummies = X_train[keep_columns].values\n",
    "# no_dummies.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalize the data \n",
    "from sklearn.preprocessing import StandardScaler #normalization \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(no_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new object with scaled data \n",
    "scaled_data = scaler.transform(no_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Explained Variance:  0.9359726222999394\n",
      "Variance per PC:  [0.13664569 0.13557782 0.13341811 0.12841654 0.07039171 0.06859363\n",
      " 0.06722037 0.0660819  0.06550855 0.06411831]\n"
     ]
    }
   ],
   "source": [
    "#Finding Components and variance explained\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 10)\n",
    "X = pca.fit_transform(scaled_data)\n",
    "print('Total Explained Variance: ', pca.explained_variance_ratio_.sum())\n",
    "print('Variance per PC: ', pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.117264</td>\n",
       "      <td>0.860249</td>\n",
       "      <td>-3.267951</td>\n",
       "      <td>-1.514983</td>\n",
       "      <td>-0.848635</td>\n",
       "      <td>-0.662204</td>\n",
       "      <td>-0.184492</td>\n",
       "      <td>-0.354839</td>\n",
       "      <td>-0.288507</td>\n",
       "      <td>-0.057933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.534473</td>\n",
       "      <td>-1.771828</td>\n",
       "      <td>0.220257</td>\n",
       "      <td>-2.372565</td>\n",
       "      <td>-0.104382</td>\n",
       "      <td>1.877681</td>\n",
       "      <td>0.018228</td>\n",
       "      <td>-0.185421</td>\n",
       "      <td>-0.989952</td>\n",
       "      <td>-0.383239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.111025</td>\n",
       "      <td>-2.534659</td>\n",
       "      <td>-2.640587</td>\n",
       "      <td>-0.846140</td>\n",
       "      <td>0.974067</td>\n",
       "      <td>-0.181060</td>\n",
       "      <td>-0.695494</td>\n",
       "      <td>0.463899</td>\n",
       "      <td>-0.011414</td>\n",
       "      <td>0.556300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.249261</td>\n",
       "      <td>-1.242818</td>\n",
       "      <td>0.047066</td>\n",
       "      <td>-0.926719</td>\n",
       "      <td>-0.521447</td>\n",
       "      <td>0.188170</td>\n",
       "      <td>-1.870896</td>\n",
       "      <td>-0.398013</td>\n",
       "      <td>0.059174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.837456</td>\n",
       "      <td>1.002739</td>\n",
       "      <td>-1.367623</td>\n",
       "      <td>2.167416</td>\n",
       "      <td>-0.471698</td>\n",
       "      <td>2.252486</td>\n",
       "      <td>-0.057525</td>\n",
       "      <td>-0.656867</td>\n",
       "      <td>-1.621744</td>\n",
       "      <td>0.047067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         1         2         3         4         5         6         7   \\\n",
       "0  1.117264  0.860249 -3.267951 -1.514983 -0.848635 -0.662204 -0.184492   \n",
       "1 -0.534473 -1.771828  0.220257 -2.372565 -0.104382  1.877681  0.018228   \n",
       "2 -0.111025 -2.534659 -2.640587 -0.846140  0.974067 -0.181060 -0.695494   \n",
       "3  0.002757  0.249261 -1.242818  0.047066 -0.926719 -0.521447  0.188170   \n",
       "4  1.837456  1.002739 -1.367623  2.167416 -0.471698  2.252486 -0.057525   \n",
       "\n",
       "         8         9         10  \n",
       "0 -0.354839 -0.288507 -0.057933  \n",
       "1 -0.185421 -0.989952 -0.383239  \n",
       "2  0.463899 -0.011414  0.556300  \n",
       "3 -1.870896 -0.398013  0.059174  \n",
       "4 -0.656867 -1.621744  0.047067  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating df with PC and known labels\n",
    "pc_df = pd.DataFrame(data = X, \n",
    "        columns = range(1,11,1))\n",
    "# pc_df.head(n = 20)\n",
    "pc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine PC and categorical data for final df \n",
    "add_columns = []\n",
    "for column in X_train: \n",
    "    if column not in keep_columns:\n",
    "        add_columns.append(column)\n",
    "len(add_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_X = X_train[add_columns].values\n",
    "add_pc = pc_df.values\n",
    "\n",
    "nodummies_pca_data_X= np.column_stack((add_X,add_pc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dummy then pca Log reg\n",
    "lr = LogisticRegression(solver = 'lbfgs')\n",
    "lr.fit(dummy_pca_data_X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8765714285714286"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(dummy_pca_data_X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pca then dummy log reg \n",
    "lr = LogisticRegression(solver = 'lbfgs')\n",
    "lr.fit(nodummies_pca_data_X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8757142857142857"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(nodummies_pca_data_X, y_train) # Same thing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': ['auto'],\n",
    "#     'min_samples_leaf': [3, 4, 5],\n",
    "#     'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(rf, param_grid, cv=10, scoring='accuracy', return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy then pca\n",
    "grid.fit(dummy_pca_data_X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "import pickle\n",
    "filename = 'Churn_RF1.sav'\n",
    "pickle.dump(grid, open(filename, 'wb'))\n",
    "grid.score(dummy_pca_data_X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca then dummy \n",
    "grid.fit(nodummies_pca_data_X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "import pickle\n",
    "filename = 'Churn_RF2.sav'\n",
    "pickle.dump(grid, open(filename, 'wb'))\n",
    "grid.score(nodummies_pca_data_X, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
